{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import Dataset\n",
    "from convnet import *\n",
    "import datetime\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Testing TF GPU Support\n",
    "\n",
    "# Creates a graph.\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "test_sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(test_sess.run(c))\n",
    "test_sess.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ds.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset /home/mostafa/BigSpiroData/train/raw_augmented_np/6/data_22.npy (sync) \n",
      "Loaded dataset /home/mostafa/BigSpiroData/train/raw_augmented_np/6/data_22.npy\n",
      "Loading dataset /home/mostafa/BigSpiroData/train/raw_augmented_np/6/data_11.npy (async) \n",
      "Loaded dataset /home/mostafa/BigSpiroData/train/raw_augmented_np/6/data_11.npy\n"
     ]
    }
   ],
   "source": [
    "root_dataset_path = '/home/mostafa/BigSpiroData/'\n",
    "data_manager = Dataset.DatasetManager(\n",
    "    train= root_dataset_path + 'train/raw_augmented_np/6/',\n",
    "    test = root_dataset_path + 'test/raw_np/6/', \n",
    "    target_shape=(85, 85, 85)\n",
    ")\n",
    "\n",
    "#data_manager = Dataset.DatasetManager(train=None, test=None, target_shape=(128, 128, 128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85, 85, 85]\n",
      "Input to Conv1 (?, 85, 85, 85, 1)\n",
      "Output of Conv1 (?, 85, 85, 85, 32)\n",
      "Input to MaxPool1 (?, 85, 85, 85, 32)\n",
      "Output of MaxPool1 (?, 40, 40, 40, 32)\n",
      "Input to Conv2 (?, 40, 40, 40, 32)\n",
      "Output of Conv2 (?, 40, 40, 40, 32)\n",
      "Input to MaxPool2 (?, 40, 40, 40, 32)\n",
      "Output to MaxPool2 (?, 17, 17, 17, 32)\n",
      "Loading dataset /home/mostafa/BigSpiroData/test/raw_np/6/data_0.npy (sync) \n",
      "/home/mostafa/BigSpiroData/test/raw_np/6/data_0.npy\n",
      "(12, 614125)\n",
      "Loaded dataset /home/mostafa/BigSpiroData/test/raw_np/6/data_0.npy\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "ds = data_manager.get_current_dataset()\n",
    "\n",
    "# Network Parameters\n",
    "data_shape = [ds.original_X_shape[1], ds.original_X_shape[2], ds.original_X_shape[3]]\n",
    "n_input = ds.original_X_shape[1] * ds.original_X_shape[2] * ds.original_X_shape[3]   # Input size\n",
    "n_classes = 2     # Classes (No Emphysema, Emphysema)\n",
    "dropout = 0.75    # Dropout, probability to keep units\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 400000\n",
    "batch_size = 3\n",
    "display_step = 10\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "\n",
    "with tf.name_scope('dropout'):\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "    tf.scalar_summary('dropout_keep_probability', keep_prob)\n",
    "    \n",
    "    \n",
    "# Number of neurons = Output Volume Size \n",
    "# Number of biases = 1 Per neuron\n",
    "# Number of Bias terms = Number of Filters \n",
    "# Number of weights = FilterWidth*Fitlerheight*FiltherDepth*InputColors\n",
    "# Output Volume Size = (FilterWidth * padding * padding )\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # Filter Width, Filter Height, Filter Slices, Filter Depth (Image channels), Filter Count\n",
    "\n",
    "    # 96 Filters of shape 5x5x5x1\n",
    "    'wc1': tf.Variable(tf.random_normal([3, 3, 3, 1, 32])),\n",
    "    \n",
    "    # Due to padding the output size remains the same @ 85x85x85x48\n",
    "    # maxpool3d with k = 7 and stride= 2\n",
    "    # Output of maxpool3d = ( (85-6)/2 )  + 1 = 40\n",
    "    # Output volume 40x40x40x96 (Filter number remains the same after maxpooling)\n",
    "    'mp1': {'k':7, 's':2},\n",
    "    \n",
    "    # 12 Filters of shape 5x5x5x96\n",
    "    'wc2': tf.Variable(tf.random_normal([3, 3, 3, 32, 32])),\n",
    "\n",
    "    'mp2': {'k':6, 's':2},\n",
    "    \n",
    "    # Due to padding the output size remains the same @ 40x40x40x48\n",
    "    # maxpool3d with k = 6 and stride= 2\n",
    "    # Output of maxpool3d = ( (40-6)/2 ) + 1 = 17\n",
    "    # Output volume 20x20x20x12 (Filter number remains the same after maxpooling)\n",
    "\n",
    "    \n",
    "    # fully connected, 8*8*12 inputs, 256 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([17*17*17*32, 256])),\n",
    "    # 256 inputs, 2 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([256, n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([32])),\n",
    "    'bd1': tf.Variable(tf.random_normal([256])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(x, data_shape, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    with tf.name_scope('total'):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "        tf.scalar_summary('cost', cost)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "with tf.name_scope('accuracy'):\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    tf.scalar_summary('accuracy', accuracy)\n",
    "\n",
    "# Initializing the variables\n",
    "merged = tf.merge_all_summaries()\n",
    "\n",
    "str_now = datetime.datetime.now().strftime(\"%d-%m-%Y#%H_%M_%S\")\n",
    "\n",
    "train_writer = tf.train.SummaryWriter('logs/train/' + str_now, sess.graph)\n",
    "test_writer  = tf.train.SummaryWriter('logs/test/'  + str_now, sess.graph)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = data_manager.get_test_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.050311606377363205"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 8 * 25 * (64 * 64 * 64 * 1 ) + (5 * 5 * 5 * 1 * 12 ) + (5 * 5 * 5 * 12 * 12) + (8 * 8 * 8 * 12 * 256) + (256 * 2)\n",
    "\n",
    "x / (1024 * 1024  * 1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 30, Minibatch Loss = 596521.187500, Training Accuracy = 1.00000, Test Accuracy = 1.00000\n",
      "Iter 60, Minibatch Loss = 179687.343750, Training Accuracy = 0.33333, Test Accuracy = 1.00000\n",
      "Iter 90, Minibatch Loss = 838276.000000, Training Accuracy = 0.00000, Test Accuracy = 0.00000\n",
      "Iter 120, Minibatch Loss = 781900.000000, Training Accuracy = 1.00000, Test Accuracy = 1.00000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-3de3f19981b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# Run optimization op (backprop)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmerged\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mtrain_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mostafa/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 382\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    383\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mostafa/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 655\u001b[1;33m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mostafa/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 723\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    724\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/mostafa/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    728\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mostafa/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    710\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "step = 1\n",
    "\n",
    "with sess:\n",
    "    \n",
    "    test_batch_x, test_batch_y = test_dataset.next_batch(4)\n",
    "    test_dict = {\n",
    "        x: test_batch_x,\n",
    "        y: test_batch_y,\n",
    "        keep_prob: 1.0\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "\n",
    "        batch_x, batch_y = data_manager.next_batch(batch_size)\n",
    "        train_dict = {\n",
    "            x: batch_x, \n",
    "            y: batch_y, \n",
    "            keep_prob: 1.0\n",
    "        }\n",
    "                        \n",
    "        # Run optimization op (backprop)\n",
    "        summary, _ = sess.run([merged, optimizer], feed_dict=train_dict)\n",
    "        train_writer.add_summary(summary, step)\n",
    "\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            train_acc, train_loss = sess.run([accuracy, cost], feed_dict=train_dict)\n",
    "            summary, test_acc  = sess.run([merged, accuracy], feed_dict=test_dict)\n",
    "\n",
    "            test_writer.add_summary(summary, step)\n",
    "            \n",
    "            print( \"Iter \" + str(step * batch_size) + \n",
    "                \", Minibatch Loss = {:.6f}\".format(train_loss) + \n",
    "                \", Training Accuracy = {:.5f}\".format(train_acc) + \n",
    "                \", Test Accuracy = {:.5f}\".format(test_acc)\n",
    "             )\n",
    "\n",
    "        step += 1\n",
    "        \n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "train_writer.close()\n",
    "test_writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "test_dataset = data_manager.get_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a batch of training examples\n",
    "test_batch_x, test_batch_y = test_dataset.next_batch(5)\n",
    "\n",
    "# Run test\n",
    "print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: test_batch_x, y: test_batch_y, keep_prob: 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
